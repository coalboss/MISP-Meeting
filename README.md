# MISP-Meeting: Multimodal Dataset for Long-Form Meeting Transcription and Summarization

**MISP-Meeting** is a realâ€‘world Mandarin meeting corpus that combines **speech, panoramic video, and text** to enable research on automatic meeting transcription and summarization (AMTS) and other multimodal perception tasks.

```
ğŸ“…  120+ hours   ğŸ™ï¸ 8â€‘ch farâ€‘field audio   ğŸ§ perâ€‘speaker headâ€‘set audio   ğŸ¥ 360Â° video
ğŸ“ sentenceâ€‘level transcripts   ğŸ“„ humanâ€‘refined brief & detailed summaries
```

> Accepted at **ACLÂ 2025** (Resource Track).  
> If you use MISPâ€‘Meeting in your work, please cite our paper (see below).
>
> ## âœ¨ Key Features
| Category | Details |
|----------|---------|
| **Scale** | 125.15Â h raw audioâ€‘visual recordings; 163 real meetings; 274 speakers; 23 rooms |
| **Modalities** | 8â€‘channel circular microphone array, perâ€‘speaker nearâ€‘field mic, 360Â° RGB video |
| **Noise Conditions** | Real meeting rooms with typing, door slams, fan noise, crossâ€‘talk |
| **Metadata** | Room geometry, speaker demographics (age, profession), topic labels |
| **Human Labels** | Sentence boundaries (â‰¤â€¯Â±100â€¯ms), manual transcripts (>â€¯99â€¯% acc.), 2â€‘pass expert summaries |
| **Licensing** | CCÂ BYâ€‘NCâ€‘NDÂ 4.0 (researchâ€‘only, free upon authorisation) |

## ğŸ”— Download
1. **Sign the licence** on the [official dataset page](https://challenge.xfyun.cn/misp_dataset).
2. Files are hosted on an OSS mirror supporting `aria2c`/`wget`.
3. Verify integrity with the provided checksums.

> Problems? Open anÂ [issue](../../issues) or eâ€‘mail the maintainers.

## âš™ï¸ Quick Start
### 1. Environment
```bash
conda create -n misp python=3.9 pytorch torchaudio cudatoolkit=11.8 -c pytorch -y
conda activate misp
pip install -r requirements.txt
```

### 2. Speech Enhancement (GSS)
We use the **guided source separation (GSS)** implementation from  
<https://github.com/desh2608/gss>.

```bash
git clone https://github.com/desh2608/gss external/gss
# follow the GSS README to enhance 8â€‘ch recordings, e.g.
python external/gss/apply_gss.py    --audio_dir /path/to/MISP/far_audio    --rt60 0.3 --mic_format misp    --out_dir /path/to/MISP/far_audio_gss
```

### 3. Speech Recognition (AVSR & Fineâ€‘tuning)
For recognition we reuse the **AVSR** recipe developed in our previous work  
<https://github.com/mispchallenge/MISP-ICME-AVSR>.

```bash
git clone https://github.com/mispchallenge/MISP-ICME-AVSR external/avsr
# see external/avsr/README.md for endâ€‘toâ€‘end training / inference
```

### 4. Summarisation Utilities
```bash
# generate brief & detailed summaries with DeepSeek LLM
python summary_by_deepseek.py
```
Alternative LLM backâ€‘ends are available: `gemini`, `kimi`, `ollama`, `qwen`.
---


## ğŸ“„ Citation
```
@inproceedings{chen2025mispmeeting,
  title     = {MISPâ€‘Meeting: A Realâ€‘World Dataset with Multimodal Cues for Longâ€‘form Meeting Transcription and Summarization},
  author    = {HangÂ Chen and JunÂ Du and SabatoÂ MarcoÂ Siniscalchi and etÂ al.},
  booktitle = {Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL)},
  year      = {2025}
}
```

## ğŸ™ Acknowledgements
MISPâ€‘Meeting is a joint effort of USTC, NVIDIA Research, UCLA, and the University of Palermo.  
We thank the 60+ research groups who provided early feedback and the volunteers who annotated and validated the corpus.
